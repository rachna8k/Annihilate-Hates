{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "train-tfidf"
      ],
      "metadata": {
        "id": "HvSygSUaER3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCrFxT8RSr6g",
        "outputId": "7867fbb1-495d-4e8e-a4f4-74e8cf0cc0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot --upgrade\n",
        "# to convert emojis to text\n",
        "!pip install emoji\n",
        "#to expand a contracted words\n",
        "!pip install demoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asZkODAFFfqg",
        "outputId": "52d07f62-6e2f-4771-c354-ee7382ead1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=33b0611cc3062fc4ba5f8be2ac5f6f038e187386975e144d277c2ac05ff996f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.7.0\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "import emot\n",
        "import nltk.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import emoji\n",
        "import demoji\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "pd.set_option(\"max_colwidth\" ,220)\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwvVljklFfnp",
        "outputId": "3ac8898e-ef80-49d2-f9c0-f3275bd0c622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP61_mV0FpEd",
        "outputId": "20fefe12-40b2-4acf-b17d-5c428275fe84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e00aa54160fe>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  try:\n",
        "   temp=emoji.demojize(str(text),delimiters=(\" \",\" \"))\n",
        "   temp=temp.replace(\"_\",\"  \")\n",
        "   return temp\n",
        "  except IndexError:\n",
        "        return \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YExerUiIFpCB",
        "outputId": "4c85feae-003c-40ce-8967-7a115f741875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-82a5fb4117d1>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punct=\"!#$%&\\'()*+,-.’’/:;<=>?@[\\\\]^_`{|}~’“‘\""
      ],
      "metadata": {
        "id": "im0gK8JVFo-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bIrCTt9rFo74",
        "outputId": "0ff23f7d-3672-49f2-fa8c-c0d1eec9707d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', str(text))"
      ],
      "metadata": {
        "id": "I1VqZlqSFytZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('@[a-zA-Z0-9]*', '', str(text))#to remove @ and its following word\n",
        "    text = contractions.fix(text, slang=True)\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #to remove punctuation\n",
        "    text=\"\".join([word.lower() for word in text if word not in punct])\n",
        "    text = \"\".join([word for word in text if not word.isdigit()])#to remove digit\n",
        "    text = \" \".join(word for word in text.split() if word not in stopwords)\n",
        "    return text"
      ],
      "metadata": {
        "id": "jGZk04rmFyoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "8djmBUajdF1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3se8x9GSgDL"
      },
      "outputs": [],
      "source": [
        "bodo_train=pd.read_csv('/content/drive/MyDrive/hasoc/task4/bodo/train_BO_AH_HASOC2023.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bodo_train['task_1'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibMJt_eviga",
        "outputId": "ce69ec86-b29e-492b-fc57-292aa22f7b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HOF    998\n",
              "NOT    681\n",
              "Name: task_1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bodo_train['clean_text'] = bodo_train['text'].apply(lambda x:emo(x))\n",
        "bodo_train['clean_text'] = bodo_train['clean_text'].apply(lambda x:remove_urls(x))\n",
        "bodo_train['clean_text'] = bodo_train['clean_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "uAg8FJS5FyiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bodo_train['clean_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKRBnDbw1SBx",
        "outputId": "79bac8ad-d5a7-4ef6-f163-ae0603ee5ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                                 गोदाव खामानि मावओ बोला नो सानसे देरहा थारगोन\n",
              "1                                                                                                                      निखावरि सुबुंफोरा सिखाव\n",
              "2                                                                                                   मा बिमा खर परिबर्थननि खोथा फैखो बेयाव मोसौ\n",
              "3                                                                                                      थोद जामबा सैमा साला मा मिसेस जाखो बेलाय\n",
              "4                                                      माखौ बकिबाय थादों नों बोरमा फानथा दम दंब्ला खामानि मावना दिन्थि बिनि उनाव बखि सैमा साला\n",
              "                                                                         ...                                                                  \n",
              "1674                                                                   नोंलाय जामबा नोंबो सासे सनमान गैयै मानसिसो गिदिरसिन मानसिखौ मुं मुखग्रा\n",
              "1675    एै मावजि लाब गैया दानो बनद खालामनायनि खोथा बुंबानो गाबोननो गरनैसो होलाय होनबानो जाबाय लागाव बुंग्रासो जागोन नोंसोरबायदि मावजि सादुफोरा\n",
              "1676                                                                                                                 सिखला फुरकव रपे खलामनांगव\n",
              "1677                                                                                                             सेनदेल खुबै नांगौलै सालाफोरखौ\n",
              "1678                                                  राकेस बोरमा सिखावजोंनो थाफाहैदोलै नोंबो आदै जानला मोनला जों जानला मोनला मिलायखायो नोंसोर\n",
              "Name: clean_text, Length: 1679, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(bodo_train.clean_text, bodo_train.task_1, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "9E1qn3MQ36ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXieK6og36gB",
        "outputId": "6d65e110-e0e6-458e-d974-5626200c50b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1259,), (420,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "def custom_analyzer(text):\n",
        "    words = regex.findall(r'\\w{2,}', text) #extract words of at least 2 letters\n",
        "#     for w in words:\n",
        "    for w in words:\n",
        "      yield w"
      ],
      "metadata": {
        "id": "x9Fdd1lXFylS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "# word level tf-idf\n",
        "tfidf_vect1 = TfidfVectorizer(analyzer=custom_analyzer)\n",
        "tfidf_vect1.fit(X_train)\n",
        "xtrain_tfidf1 =  tfidf_vect1.transform(X_train)\n",
        "xtest_tfidf1=  tfidf_vect1.transform(X_test)"
      ],
      "metadata": {
        "id": "0W3gYJWbG_CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_tfidf1 .shape,xtest_tfidf1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G23JZ5Mc3mF_",
        "outputId": "dea18702-266f-4052-a74f-730912fc49bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1259, 3844), (420, 3844))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "\n",
        "svm .fit(xtrain_tfidf1, y_train)\n",
        "\n",
        "y_pred = svm.predict(xtest_tfidf1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "tEJDq78Z2UyB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4161f47b-272e-4210-8944-aa2a4bf965a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 76.9048\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.73      0.94      0.82       233\n",
            "         NOT       0.88      0.56      0.68       187\n",
            "\n",
            "    accuracy                           0.77       420\n",
            "   macro avg       0.80      0.75      0.75       420\n",
            "weighted avg       0.79      0.77      0.76       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = BernoulliNB()\n",
        "clf3 = LinearSVC(penalty='l2',C=1.0)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('LR', clf1), ('BNB', clf2),\n",
        "('SVC', clf3)], voting='hard')\n",
        "eclf = eclf.fit(xtrain_tfidf1, y_train)\n",
        "y_pred = eclf.predict(xtest_tfidf1)\n",
        "print(y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "V2h5w6ZgfR0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiBrjqQ_6Bw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}