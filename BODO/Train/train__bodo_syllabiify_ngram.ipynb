{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J1pr9D6KkWv",
        "outputId": "e1958fcc-96d6-4cf9-d533-50f9758d9269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "syllable n grams"
      ],
      "metadata": {
        "id": "tfoVUstSgYkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zTAZ0YXeKvby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot --upgrade\n",
        "# to convert emojis to text\n",
        "!pip install emoji\n",
        "#to expand a contracted words\n",
        "!pip install demoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n7YQnKuKxl4",
        "outputId": "fdc65373-ce7d-4c25-e0c9-3e24cc5a6571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=6176368a6c0ed0226c7ec4e76ed5d3867e4557f277d04159e17a0c5202551b3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/11/48/5df0b9727d5669c9174a141134f10304d1d78a3b89a4676f3d\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.7.0\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "import emot\n",
        "import nltk.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import emoji\n",
        "import demoji\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "pd.set_option(\"max_colwidth\" ,220)\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueEFZT2UK8T5",
        "outputId": "9d5fe207-0de6-497d-d7c2-4d8d8bfd2ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSM5vvjuK8OC",
        "outputId": "3368bbe3-eaf5-48da-b630-8e0c795b575a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-e00aa54160fe>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  try:\n",
        "   temp=emoji.demojize(str(text),delimiters=(\" \",\" \"))\n",
        "   temp=temp.replace(\"_\",\"  \")\n",
        "   return temp\n",
        "  except IndexError:\n",
        "        return \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW3wwPbYK8LI",
        "outputId": "e168b159-358e-47ad-ff6f-7ba7c5917981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-82a5fb4117d1>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punct=\"!#$%&\\'()*+,-.’’/:;<=>?@[\\\\]^_`{|}~’“‘\""
      ],
      "metadata": {
        "id": "ZLmIsmt8K76_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M4wmn2MtLcCg",
        "outputId": "fb434351-5d4c-451d-dffa-ad20fdcab55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', str(text))"
      ],
      "metadata": {
        "id": "nTN6tGlcLb_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('@[a-zA-Z0-9]*', '', str(text))#to remove @ and its following word\n",
        "    text = contractions.fix(text, slang=True)\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #to remove punctuation\n",
        "    text=\"\".join([word.lower() for word in text if word not in punct])\n",
        "    text = \"\".join([word for word in text if not word.isdigit()])#to remove digit\n",
        "    text = \" \".join(word for word in text.split() if word not in stopwords)\n",
        "    return text"
      ],
      "metadata": {
        "id": "0ostz3MSLb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bodo_train=pd.read_csv('/content/drive/MyDrive/hasoc/task4/bodo/train_BO_AH_HASOC2023.csv')\n",
        "bodo_test=pd.read_csv('/content/drive/MyDrive/hasoc/task4/bodo/test_BO_AH_HASOC2023.csv')"
      ],
      "metadata": {
        "id": "B2PWmlubLnjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bodo_train['clean_text'] = bodo_train['text'].apply(lambda x:emo(x))\n",
        "bodo_train['clean_text'] = bodo_train['clean_text'].apply(lambda x:remove_urls(x))\n",
        "bodo_train['clean_text'] = bodo_train['clean_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "rBusIb0zLnhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bodo_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKA5_vuHdnII",
        "outputId": "d078d115-6a40-44b3-a7b6-4e08b11342e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1679"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(bodo_train.clean_text, bodo_train.task_1, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "VT-x-0G1Lneg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3TZkVQCLnb5",
        "outputId": "78474f93-7492-4831-cf8e-8ade9442b3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1259,), (420,))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import  CountVectorizer, TfidfTransformer,TfidfVectorizer"
      ],
      "metadata": {
        "id": "PNcAJcNWLnZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/libindic/syllabalizer.git\n",
        "%cd syllabalizer\n",
        "%run setup.py sdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md29gM2TLnWx",
        "outputId": "18c4336c-2796-4b29-d809-2f2dbb75d9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'syllabalizer'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "Receiving objects:   0% (1/186)\rReceiving objects:   1% (2/186)\rReceiving objects:   2% (4/186)\rReceiving objects:   3% (6/186)\rReceiving objects:   4% (8/186)\rReceiving objects:   5% (10/186)\rReceiving objects:   6% (12/186)\rReceiving objects:   7% (14/186)\rReceiving objects:   8% (15/186)\rReceiving objects:   9% (17/186)\rReceiving objects:  10% (19/186)\rReceiving objects:  11% (21/186)\rReceiving objects:  12% (23/186)\rReceiving objects:  13% (25/186)\rReceiving objects:  14% (27/186)\rReceiving objects:  15% (28/186)\rReceiving objects:  16% (30/186)\rReceiving objects:  17% (32/186)\rReceiving objects:  18% (34/186)\rReceiving objects:  19% (36/186)\rReceiving objects:  20% (38/186)\rReceiving objects:  21% (40/186)\rReceiving objects:  22% (41/186)\rReceiving objects:  23% (43/186)\rReceiving objects:  24% (45/186)\rReceiving objects:  25% (47/186)\rReceiving objects:  26% (49/186)\rReceiving objects:  27% (51/186)\rReceiving objects:  28% (53/186)\rReceiving objects:  29% (54/186)\rReceiving objects:  30% (56/186)\rReceiving objects:  31% (58/186)\rReceiving objects:  32% (60/186)\rReceiving objects:  33% (62/186)\rReceiving objects:  34% (64/186)\rReceiving objects:  35% (66/186)\rReceiving objects:  36% (67/186)\rReceiving objects:  37% (69/186)\rReceiving objects:  38% (71/186)\rReceiving objects:  39% (73/186)\rReceiving objects:  40% (75/186)\rReceiving objects:  41% (77/186)\rReceiving objects:  42% (79/186)\rReceiving objects:  43% (80/186)\rReceiving objects:  44% (82/186)\rReceiving objects:  45% (84/186)\rReceiving objects:  46% (86/186)\rReceiving objects:  47% (88/186)\rReceiving objects:  48% (90/186)\rReceiving objects:  49% (92/186)\rReceiving objects:  50% (93/186)\rReceiving objects:  51% (95/186)\rReceiving objects:  52% (97/186)\rReceiving objects:  53% (99/186)\rReceiving objects:  54% (101/186)\rReceiving objects:  55% (103/186)\rReceiving objects:  56% (105/186)\rReceiving objects:  57% (107/186)\rremote: Total 186 (delta 0), reused 0 (delta 0), pack-reused 186\u001b[K\n",
            "Receiving objects: 100% (186/186), 41.35 KiB | 2.95 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'author-email' will not be supported in future\n",
            "        versions. Please use the underscore name 'author_email' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'home-page' will not be supported in future\n",
            "        versions. Please use the underscore name 'home_page' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "INFO:root:[pbr] Generating ChangeLog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/syllabalizer/syllabalizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:955: SetuptoolsDeprecationWarning: The namespace_packages parameter is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please replace its usage with implicit namespaces (PEP 420).\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  ep.load()(self, ep.name, value)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'author-email' will not be supported in future\n",
            "        versions. Please use the underscore name 'author_email' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'home-page' will not be supported in future\n",
            "        versions. Please use the underscore name 'home_page' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running sdist\n",
            "INFO:root:[pbr] Writing ChangeLog\n",
            "INFO:root:[pbr] Generating ChangeLog\n",
            "INFO:root:[pbr] ChangeLog complete (0.0s)\n",
            "INFO:root:[pbr] Generating AUTHORS\n",
            "INFO:root:[pbr] AUTHORS complete (0.0s)\n",
            "INFO:root:running egg_info\n",
            "INFO:root:creating libindic_syllabifier.egg-info\n",
            "INFO:root:writing pbr to libindic_syllabifier.egg-info/pbr.json\n",
            "INFO:root:writing libindic_syllabifier.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to libindic_syllabifier.egg-info/dependency_links.txt\n",
            "INFO:root:writing namespace_packages to libindic_syllabifier.egg-info/namespace_packages.txt\n",
            "INFO:root:writing requirements to libindic_syllabifier.egg-info/requires.txt\n",
            "INFO:root:writing top-level names to libindic_syllabifier.egg-info/top_level.txt\n",
            "INFO:root:[pbr] Processing SOURCES.txt\n",
            "INFO:root:writing manifest file 'libindic_syllabifier.egg-info/SOURCES.txt'\n",
            "INFO:root:[pbr] In git context, generating filelist from git\n",
            "WARNING:root:warning: no previously-included files found matching '.gitreview'\n",
            "WARNING:root:warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "INFO:root:writing manifest file 'libindic_syllabifier.egg-info/SOURCES.txt'\n",
            "INFO:root:[pbr] reno was not found or is too old. Skipping release notes\n",
            "INFO:root:running check\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small/static\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/templates\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:copying files to libindic-syllabifier-0.0.1.dev28...\n",
            "INFO:root:copying .testr.conf -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying .travis.yml -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying AUTHORS -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying ChangeLog -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying Makefile -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying README.md -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying circle.yml -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying requirements.txt -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying setup.cfg -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying setup.py -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying test-requirements.txt -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying tox.ini -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying docs/Makefile -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/conf.py -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/index.rst -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/make.bat -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/_themes/LICENSE -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/README.rst -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/flask_theme_support.py -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/kr/layout.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/relations.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/theme.conf -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/static/flasky.css_t -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:copying docs/_themes/kr/static/small_flask.css -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:copying docs/_themes/kr_small/layout.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:copying docs/_themes/kr_small/theme.conf -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:copying docs/_themes/kr_small/static/flasky.css_t -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small/static\n",
            "INFO:root:copying libindic/syllabifier/__init__.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:copying libindic/syllabifier/core.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:copying libindic/syllabifier/templates/libindic.syllabifier.html -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/templates\n",
            "INFO:root:copying libindic/syllabifier/tests/__init__.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:copying libindic/syllabifier/tests/test_indicsyllabifier.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:copying libindic_syllabifier.egg-info/PKG-INFO -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/SOURCES.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/dependency_links.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/namespace_packages.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/not-zip-safe -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/pbr.json -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/requires.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/top_level.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying tests/__init__.py -> libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:copying tests/indcsyllabifier_test.py -> libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:Writing libindic-syllabifier-0.0.1.dev28/setup.cfg\n",
            "INFO:root:creating dist\n",
            "INFO:root:Creating tar archive\n",
            "INFO:root:removing 'libindic-syllabifier-0.0.1.dev28' (and everything under it)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dist/libindic-syllabifier*.tar.gz\n",
        "!pip install libindic-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3tCAUdVLnRY",
        "outputId": "079b3a6a-6a3f-481e-f41d-06374a405c56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./dist/libindic-syllabifier-0.0.1.dev28.tar.gz\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: silpa_common in /usr/local/lib/python3.10/dist-packages (from libindic-syllabifier==0.0.1.dev28) (0.3)\n",
            "Building wheels for collected packages: libindic-syllabifier\n",
            "  Building wheel for libindic-syllabifier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libindic-syllabifier: filename=libindic_syllabifier-0.0.1.dev28-py2.py3-none-any.whl size=8121 sha256=f45ef2211d3cf2e66712ce6e3a6845779803d27bdd9ffd257d96fb44ccaddca4\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/3a/dc/a53cc7d164af7329d22c07766872eba71c879984ce2d29b057\n",
            "Successfully built libindic-syllabifier\n",
            "Installing collected packages: libindic-syllabifier\n",
            "  Attempting uninstall: libindic-syllabifier\n",
            "    Found existing installation: libindic-syllabifier 0.0.1.dev28\n",
            "    Uninstalling libindic-syllabifier-0.0.1.dev28:\n",
            "      Successfully uninstalled libindic-syllabifier-0.0.1.dev28\n",
            "Successfully installed libindic-syllabifier-0.0.1.dev28\n",
            "Requirement already satisfied: libindic-utils in /usr/local/lib/python3.10/dist-packages (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from libindic.syllabifier import Syllabifier\n",
        "instance = Syllabifier()"
      ],
      "metadata": {
        "id": "7tzUFkWJLnOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable(word):\n",
        "    try:\n",
        "        sylbls = instance.syllabify(word)\n",
        "    except:\n",
        "        sylbls = list(word)\n",
        "    return sylbls"
      ],
      "metadata": {
        "id": "KtgATuZfM1y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate word n-grams\n",
        "\n",
        "def generate_N_grams(text, ngram=1):\n",
        "    words = [word for word in text.split(\" \")]  # if word not in set(stopwords.words('english'))]\n",
        "    temp = zip(*[words[i:] for i in range(0, ngram)])\n",
        "    ngrams = [' '.join(ngram) for ngram in temp]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "--zdcWzrM1v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "\n",
        "def custom_analyzer(text):\n",
        "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
        "    for w in words:\n",
        "        yield w"
      ],
      "metadata": {
        "id": "4U90T1fXM1tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate syllable n-grams by specifying the value of n, by default ngram=1\n",
        "\n",
        "def generate_cN_grams(word, ngram=1):\n",
        "    sylbs = [sylbs for sylbs in syllable(word)]  # if word not in set(stopwords.words('english'))]\n",
        "    temp = zip(*[sylbs[i:] for i in range(0, ngram)])\n",
        "    ngrams = [''.join(ngram) for ngram in temp]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "ZvURcs8zM1qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate syllable n-grams in the range 'from' to 'to'. by default 'from'= 1\n",
        "\n",
        "def ngrams(text, r_to, r_from=1):\n",
        "    r_from = 1\n",
        "    r_to = 3\n",
        "    allngrams=[]\n",
        "    for word in text.split():\n",
        "        temp = []\n",
        "        for i in range(r_from + r_to - 1):\n",
        "            temp.extend(generate_cN_grams(word, i+1))\n",
        "            allngrams.append(temp)\n",
        "    n_grams = [item for sublist in allngrams for item in sublist]\n",
        "  # merged = list(itertools.chain(allngrams))\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "lZfZmiLUM1oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_analyzer(text):\n",
        "    syngrams = ngrams(text,3)\n",
        "    return syngrams"
      ],
      "metadata": {
        "id": "vmFsgPruM1lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(analyzer=custom_analyzer)\n",
        "tfidf_vectorizer.fit_transform(X_train)\n",
        "xtrain_tfidf =  tfidf_vectorizer.transform(X_train)\n",
        "xtest_tfidf =  tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "4eVa_l_fM1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "WsNMDFxuM1aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm, tree\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "B4WpNg6DNYYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers=[]"
      ],
      "metadata": {
        "id": "3QCeQKr0NYVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNB = MultinomialNB()\n",
        "classifiers.append(MNB)\n",
        "SVM = svm.SVC()\n",
        "classifiers.append(SVM)\n",
        "DT = tree.DecisionTreeClassifier()\n",
        "classifiers.append(DT)\n",
        "RFC = RandomForestClassifier()\n",
        "classifiers.append(RFC)\n",
        "LR = LogisticRegression()\n",
        "classifiers.append(LR)"
      ],
      "metadata": {
        "id": "5h-1heeeNYSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "Pdpz9HkgNYPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clf in classifiers:\n",
        "    clf.fit(xtrain_tfidf, y_train_encoded)\n",
        "    y_pred= clf.predict(xtest_tfidf)\n",
        "    acc = accuracy_score(y_test_encoded, y_pred)\n",
        "    print(\"Accuracy of %s is %s\"%(clf, acc))\n",
        "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
        "    print(\"Confusion Matrix of %s is %s\"%(clf, cm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Lg0_dgNYNA",
        "outputId": "aa155ca8-39ab-46c3-e366-fb8cf1817cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of MultinomialNB() is 0.6952380952380952\n",
            "Confusion Matrix of MultinomialNB() is [[232   1]\n",
            " [127  60]]\n",
            "Accuracy of SVC() is 0.7976190476190477\n",
            "Confusion Matrix of SVC() is [[223  10]\n",
            " [ 75 112]]\n",
            "Accuracy of DecisionTreeClassifier() is 0.6976190476190476\n",
            "Confusion Matrix of DecisionTreeClassifier() is [[177  56]\n",
            " [ 71 116]]\n",
            "Accuracy of RandomForestClassifier() is 0.7857142857142857\n",
            "Confusion Matrix of RandomForestClassifier() is [[220  13]\n",
            " [ 77 110]]\n",
            "Accuracy of LogisticRegression() is 0.7857142857142857\n",
            "Confusion Matrix of LogisticRegression() is [[220  13]\n",
            " [ 77 110]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = BernoulliNB()\n",
        "clf3 = LinearSVC(penalty='l2',C=1.0)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('LR', clf1), ('BNB', clf2),\n",
        "('SVC', clf3)], voting='hard')\n",
        "eclf = eclf.fit(xtrain_tfidf, y_train)\n",
        "y_pred_df= eclf.predict(xtest_tfidf)\n",
        "print(y_pred_df)\n",
        "\n",
        "print(\"Accuracy of %s is %s\"%(clf, acc))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM3Ca_lRWboW",
        "outputId": "45af563c-141b-4554-eb66-8a3cf482293f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT'\n",
            " 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'NOT' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT'\n",
            " 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'NOT'\n",
            " 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT'\n",
            " 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF'\n",
            " 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'NOT' 'NOT' 'NOT' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'NOT' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT'\n",
            " 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF'\n",
            " 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF'\n",
            " 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF'\n",
            " 'NOT' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF' 'NOT'\n",
            " 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF' 'NOT' 'HOF'\n",
            " 'NOT' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'NOT' 'NOT'\n",
            " 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'HOF' 'NOT' 'HOF' 'HOF']\n",
            "Accuracy of LogisticRegression() is 0.7857142857142857\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.76      0.94      0.84       233\n",
            "         NOT       0.89      0.63      0.74       187\n",
            "\n",
            "    accuracy                           0.80       420\n",
            "   macro avg       0.83      0.78      0.79       420\n",
            "weighted avg       0.82      0.80      0.79       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "\n",
        "svm .fit(xtrain_tfidf, y_train)\n",
        "\n",
        "y_pred = svm.predict(xtest_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg3Ls_WHNYJ4",
        "outputId": "eb22d78a-3d52-436d-de1c-3ef6601539cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.7619\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         HOF       0.75      0.96      0.84       233\n",
            "         NOT       0.92      0.60      0.72       187\n",
            "\n",
            "    accuracy                           0.80       420\n",
            "   macro avg       0.83      0.78      0.78       420\n",
            "weighted avg       0.82      0.80      0.79       420\n",
            "\n"
          ]
        }
      ]
    }
  ]
}