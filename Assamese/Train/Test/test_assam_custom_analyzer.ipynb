{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assamese -custom analyzer"
      ],
      "metadata": {
        "id": "cgItcjJCyqWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcl3byBtPjpr",
        "outputId": "7b35325d-e7e7-43c9-d33e-c6da8c82463b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Sb2HdmhPP70V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot --upgrade\n",
        "# to convert emojis to text\n",
        "!pip install emoji\n",
        "#to expand a contracted words\n",
        "!pip install demoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "e4YRqtSYQNEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1931766-c534-4f0d-e1e7-ccd66dbe5e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m61.4/61.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n",
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from textblob import TextBlob\n",
        "import emot\n",
        "import nltk.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import emoji\n",
        "import demoji\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "pd.set_option(\"max_colwidth\" ,220)\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "LD432Z6TQNBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e8563f-7491-4722-ab27-98f692912fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopwords = stopwords.words(\"english\")\n",
        "with open('/content/drive/MyDrive/hasoc/task4/assam/assamese.txt', encoding = 'utf-8') as f:\n",
        "    assam_stopwords = f.readlines()\n",
        "    for i in range(len(assam_stopwords)):\n",
        "        assam_stopwords[i] = re.sub('\\n','', assam_stopwords[i])\n",
        "stopwords = english_stopwords + assam_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU5KsSW8xh_O",
        "outputId": "3feb7203-cd8d-4d06-d023-7e29aaeb9011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m5SLcSQxwwk",
        "outputId": "c84252dd-2179-4a30-e092-07eb5d16e1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "295"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjMEW6wxQM99",
        "outputId": "b51d926e-6375-4988-cc72-0b7f637ec3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e00aa54160fe>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  try:\n",
        "   temp=emoji.demojize(str(text),delimiters=(\" \",\" \"))\n",
        "   temp=temp.replace(\"_\",\"  \")\n",
        "   return temp\n",
        "  except IndexError:\n",
        "        return \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIYPETzGQM4e",
        "outputId": "46737627-f5af-491c-c799-972df3ab4ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-82a5fb4117d1>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punct=\"!#$%&\\'()*+,-.’’/:;<=>?@[\\\\]^_`{|}~’“‘\""
      ],
      "metadata": {
        "id": "uomrCGccyauN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_oxZg2YAyXXE",
        "outputId": "3221083e-8c45-4e56-cb2e-481e51b97810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', str(text))"
      ],
      "metadata": {
        "id": "4ASaSy8iQpCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('@[a-zA-Z0-9]*', '', str(text))#to remove @ and its following word\n",
        "    text = contractions.fix(text, slang=True)\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #to remove punctuation\n",
        "    text=\"\".join([word.lower() for word in text if word not in punct])\n",
        "    text = \"\".join([word for word in text if not word.isdigit()])#to remove digit\n",
        "    text = \" \".join(word for word in text.split() if word not in stopwords)\n",
        "    return text"
      ],
      "metadata": {
        "id": "3E7OHVlHQsJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asa_train=pd.read_csv('/content/drive/MyDrive/hasoc/task4/assam/train_A_AH_HASOC2023.csv')\n",
        "asa_test=pd.read_csv('/content/drive/MyDrive/hasoc/task4/assam/test_A_AH_HASOC2023.csv')"
      ],
      "metadata": {
        "id": "FMsKiteiQtT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asa_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_0LVh_a0RBD",
        "outputId": "4eb92f22-de01-4872-f861-0b43e5b2365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4036, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "asa_train['task_1'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gFYfhBbQtOe",
        "outputId": "47e0a35e-97fd-44f1-95fa-461436cd6f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HOF    2347\n",
              "NOT    1689\n",
              "Name: task_1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asa_train['clean_text'] = asa_train['text'].apply(lambda x:emo(x))\n",
        "asa_train['clean_text'] = asa_train['clean_text'].apply(lambda x:remove_urls(x))\n",
        "asa_train['clean_text'] = asa_train['clean_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "xv7WC-2EQtMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asa_test['clean_text'] = asa_test['text'].apply(lambda x:emo(x))\n",
        "asa_test['clean_text'] = asa_test['clean_text'].apply(lambda x:remove_urls(x))\n",
        "asa_test['clean_text'] = asa_test['clean_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "t7eL9skituq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=asa_train['clean_text']\n",
        "X_test=asa_test['clean_text']\n",
        "y_train=asa_train['task_1']"
      ],
      "metadata": {
        "id": "ild9o2B3Hrlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3u7-T2Tvw1Z",
        "outputId": "2b0aac7d-440a-46e7-b093-e2b7e8c7b52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4036,), (1009,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.custom_analyzer"
      ],
      "metadata": {
        "id": "A-BTj346tSpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "def custom_analyzer(text):\n",
        "    words = regex.findall(r'\\w{2,}', text) #extract words of at least 2 letters\n",
        "#     for w in words:\n",
        "    for w in words:\n",
        "      yield w"
      ],
      "metadata": {
        "id": "ylvbgT8BQtGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "tfidf_vect = TfidfVectorizer(analyzer=custom_analyzer) #range unigram\n",
        "tfidf_vect.fit(X_train)\n",
        "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
        "xtest_tfidf =  tfidf_vect.transform(X_test)"
      ],
      "metadata": {
        "id": "g2KEoWSxHxF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(xtrain_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_3NEL8LH0q_",
        "outputId": "1363b410-6855-408d-f8ca-9b8f582a1d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_tfidf.shape,xtest_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkYmVMe0H-LD",
        "outputId": "3e81bce4-d12e-4eca-9345-68ebcb6333e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4036, 10385), (1009, 10385))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(xtrain_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FlLWLA-H_hO",
        "outputId": "3b601180-1fb5-4363-811b-80aac07e206d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse._csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Construction\n",
        "\n",
        "\n",
        "*   SVM\n",
        "*   Ensemble CLassifier\n",
        "\n"
      ],
      "metadata": {
        "id": "2zWgd2yltDQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "\n",
        "svm .fit(xtrain_tfidf, y_train)\n",
        "\n",
        "y_pred = svm.predict(xtest_tfidf)\n"
      ],
      "metadata": {
        "id": "DuT7290IzeoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Submission\n",
        "# to create .csv file consisting of tweet ids and predicted labels\n",
        "y_pred_df= pd.DataFrame(data=y_pred, columns=['y_pred'])\n",
        "Submisssion_assamese = pd.DataFrame()\n",
        "Submisssion_assamese['S. No.'] = asa_test['S. No.']\n",
        "Submisssion_assamese['task_1'] = y_pred_df\n",
        "Submisssion_assamese.to_csv('/content/drive/MyDrive/MUCS_test_assamese_run_2.csv',index = None)"
      ],
      "metadata": {
        "id": "zvnw68wBVG_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = BernoulliNB()\n",
        "clf3 = LinearSVC(penalty='l2',C=1.0)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('LR', clf1), ('BNB', clf2),\n",
        "('SVC', clf3)], voting='hard')\n",
        "eclf = eclf.fit(xtrain_tfidf1, y_train)\n",
        "y_pred_df= eclf.predict(xtest_tfidf1)\n",
        "print(y_pred_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2XDxygZSVQ8",
        "outputId": "fd999076-1fe6-4d4b-a21d-3155427db0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HOF' 'HOF' 'NOT' ... 'HOF' 'HOF' 'HOF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Submission\n",
        "# to create .csv file consisting of tweet ids and predicted labels\n",
        "y_pred_df1= pd.DataFrame(data=y_pred, columns=['y_pred'])\n",
        "Submisssion_assamese = pd.DataFrame()\n",
        "Submisssion_assamese['S. No.'] = asa_test['S. No.']\n",
        "Submisssion_assamese['task_1'] = y_pred_df1\n",
        "Submisssion_assamese.to_csv('/content/drive/MyDrive/MUCS_test_assamese_run_3.csv',index = None)"
      ],
      "metadata": {
        "id": "L7FPGQPCzMlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n",
        "using word TFIDF"
      ],
      "metadata": {
        "id": "kXJy65x1tajN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "tfidf_vect2 = TfidfVectorizer(analyzer='word',ngram_range=(1,3))\n",
        "tfidf_vect2.fit(X_train)\n",
        "xtrain_tfidf2 =  tfidf_vect2.transform(X_train)\n",
        "xtest_tfidf2 =  tfidf_vect2.transform(X_test)"
      ],
      "metadata": {
        "id": "3pyQHrJMIpqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "\n",
        "svm .fit(xtrain_tfidf2, y_train)\n",
        "\n",
        "y_pred = svm.predict(xtest_tfidf2)"
      ],
      "metadata": {
        "id": "8rohcquL0df5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction Using charcater"
      ],
      "metadata": {
        "id": "MSp0x0xxtsC-"
      }
    }
  ]
}