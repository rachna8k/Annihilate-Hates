{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J1pr9D6KkWv",
        "outputId": "2af31192-e589-40f7-c33e-6d305de5dbb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zTAZ0YXeKvby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot --upgrade\n",
        "# to convert emojis to text\n",
        "!pip install emoji\n",
        "#to expand a contracted words\n",
        "!pip install demoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n7YQnKuKxl4",
        "outputId": "dbc56099-cf11-471a-f366-7a5c438c8777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emot in /usr/local/lib/python3.10/dist-packages (3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: demoji in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
        "from textblob import TextBlob\n",
        "import emot\n",
        "import nltk.data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import emoji\n",
        "import demoji\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "pd.set_option(\"max_colwidth\" ,220)\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueEFZT2UK8T5",
        "outputId": "1462c731-bed5-44f8-96c4-25b7bc56114b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stopwords = stopwords.words(\"english\")\n",
        "with open('/content/drive/MyDrive/hasoc/task4/assam/assamese.txt', encoding = 'utf-8') as f:\n",
        "    assam_stopwords = f.readlines()\n",
        "    for i in range(len(assam_stopwords)):\n",
        "        assam_stopwords[i] = re.sub('\\n','', assam_stopwords[i])\n",
        "stopwords = english_stopwords + assam_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfy-tCHkK8Q8",
        "outputId": "fd4cb476-0020-49e4-86f2-8b22ad6cbd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
        "  temp=temp.replace(\"_\",\"  \")\n",
        "  return temp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSM5vvjuK8OC",
        "outputId": "e2c676be-2241-4d0e-cc79-19e2abf82662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e00aa54160fe>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demoji.download_codes()\n",
        "def emo(text):\n",
        "  try:\n",
        "   temp=emoji.demojize(str(text),delimiters=(\" \",\" \"))\n",
        "   temp=temp.replace(\"_\",\"  \")\n",
        "   return temp\n",
        "  except IndexError:\n",
        "        return \"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW3wwPbYK8LI",
        "outputId": "f672c08f-46dc-493c-d80f-08f932c69fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-82a5fb4117d1>:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punct=\"!#$%&\\'()*+,-.’’/:;<=>?@[\\\\]^_`{|}~’“‘\""
      ],
      "metadata": {
        "id": "ZLmIsmt8K76_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "M4wmn2MtLcCg",
        "outputId": "307f28b5-de05-4f9c-8f7f-bb282fb7b89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', str(text))"
      ],
      "metadata": {
        "id": "nTN6tGlcLb_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub('@[a-zA-Z0-9]*', '', str(text))#to remove @ and its following word\n",
        "    text = contractions.fix(text, slang=True)\n",
        "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #to remove punctuation\n",
        "    text=\"\".join([word.lower() for word in text if word not in punct])\n",
        "    text = \"\".join([word for word in text if not word.isdigit()])#to remove digit\n",
        "    text = \" \".join(word for word in text.split() if word not in stopwords)\n",
        "    return text"
      ],
      "metadata": {
        "id": "0ostz3MSLb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asa_train=pd.read_csv('/content/drive/MyDrive/hasoc/task4/assam/train_A_AH_HASOC2023.csv')"
      ],
      "metadata": {
        "id": "B2PWmlubLnjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asa_train['clean_text'] = asa_train['text'].apply(lambda x:emo(x))\n",
        "asa_train['clean_text'] = asa_train['clean_text'].apply(lambda x:remove_urls(x))\n",
        "asa_train['clean_text'] = asa_train['clean_text'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "rBusIb0zLnhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To split train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(asa_train.clean_text, asa_train.task_1, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "VT-x-0G1Lneg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3TZkVQCLnb5",
        "outputId": "cefb4c98-8daa-4bbf-a944-f3d68253ebe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3027,), (1009,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "feature extraction using using sllable"
      ],
      "metadata": {
        "id": "g5IVD4r9W5uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import  CountVectorizer, TfidfTransformer,TfidfVectorizer"
      ],
      "metadata": {
        "id": "PNcAJcNWLnZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/libindic/syllabalizer.git\n",
        "%cd syllabalizer\n",
        "%run setup.py sdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md29gM2TLnWx",
        "outputId": "9bcd5d77-cebe-433e-def6-6e98b140224e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'syllabalizer' already exists and is not an empty directory.\n",
            "/content/syllabalizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'author-email' will not be supported in future\n",
            "        versions. Please use the underscore name 'author_email' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'home-page' will not be supported in future\n",
            "        versions. Please use the underscore name 'home_page' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Requirements should be satisfied by a PEP 517 installer.\n",
            "        If you are using pip, you can try `pip install --use-pep517`.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  dist.fetch_build_eggs(dist.setup_requires)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:955: SetuptoolsDeprecationWarning: The namespace_packages parameter is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please replace its usage with implicit namespaces (PEP 420).\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  ep.load()(self, ep.name, value)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'description-file' will not be supported in future\n",
            "        versions. Please use the underscore name 'description_file' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'author-email' will not be supported in future\n",
            "        versions. Please use the underscore name 'author_email' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Usage of dash-separated 'home-page' will not be supported in future\n",
            "        versions. Please use the underscore name 'home_page' instead.\n",
            "\n",
            "        By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  opt = self.warn_dash_deprecation(opt, section)\n",
            "INFO:root:running sdist\n",
            "INFO:root:[pbr] Writing ChangeLog\n",
            "INFO:root:[pbr] Generating ChangeLog\n",
            "INFO:root:[pbr] ChangeLog complete (0.0s)\n",
            "INFO:root:[pbr] Generating AUTHORS\n",
            "INFO:root:[pbr] AUTHORS complete (0.0s)\n",
            "INFO:root:running egg_info\n",
            "INFO:root:writing pbr to libindic_syllabifier.egg-info/pbr.json\n",
            "INFO:root:writing libindic_syllabifier.egg-info/PKG-INFO\n",
            "INFO:root:writing dependency_links to libindic_syllabifier.egg-info/dependency_links.txt\n",
            "INFO:root:writing namespace_packages to libindic_syllabifier.egg-info/namespace_packages.txt\n",
            "INFO:root:writing requirements to libindic_syllabifier.egg-info/requires.txt\n",
            "INFO:root:writing top-level names to libindic_syllabifier.egg-info/top_level.txt\n",
            "INFO:root:[pbr] Processing SOURCES.txt\n",
            "INFO:root:[pbr] In git context, generating filelist from git\n",
            "WARNING:root:warning: no previously-included files found matching '.gitreview'\n",
            "WARNING:root:warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "INFO:root:adding license file 'AUTHORS'\n",
            "INFO:root:writing manifest file 'libindic_syllabifier.egg-info/SOURCES.txt'\n",
            "INFO:root:[pbr] reno was not found or is too old. Skipping release notes\n",
            "INFO:root:running check\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small/static\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/templates\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:creating libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:copying files to libindic-syllabifier-0.0.1.dev28...\n",
            "INFO:root:copying .testr.conf -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying .travis.yml -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying AUTHORS -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying ChangeLog -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying Makefile -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying README.md -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying circle.yml -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying requirements.txt -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying setup.cfg -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying setup.py -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying test-requirements.txt -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying tox.ini -> libindic-syllabifier-0.0.1.dev28\n",
            "INFO:root:copying docs/Makefile -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/conf.py -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/index.rst -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/make.bat -> libindic-syllabifier-0.0.1.dev28/docs\n",
            "INFO:root:copying docs/_themes/LICENSE -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/README.rst -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/flask_theme_support.py -> libindic-syllabifier-0.0.1.dev28/docs/_themes\n",
            "INFO:root:copying docs/_themes/kr/layout.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/relations.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/theme.conf -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr\n",
            "INFO:root:copying docs/_themes/kr/static/flasky.css_t -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:copying docs/_themes/kr/static/small_flask.css -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr/static\n",
            "INFO:root:copying docs/_themes/kr_small/layout.html -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:copying docs/_themes/kr_small/theme.conf -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small\n",
            "INFO:root:copying docs/_themes/kr_small/static/flasky.css_t -> libindic-syllabifier-0.0.1.dev28/docs/_themes/kr_small/static\n",
            "INFO:root:copying libindic/syllabifier/__init__.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:copying libindic/syllabifier/core.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier\n",
            "INFO:root:copying libindic/syllabifier/templates/libindic.syllabifier.html -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/templates\n",
            "INFO:root:copying libindic/syllabifier/tests/__init__.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:copying libindic/syllabifier/tests/test_indicsyllabifier.py -> libindic-syllabifier-0.0.1.dev28/libindic/syllabifier/tests\n",
            "INFO:root:copying libindic_syllabifier.egg-info/PKG-INFO -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/SOURCES.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/dependency_links.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/namespace_packages.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/not-zip-safe -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/pbr.json -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/requires.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying libindic_syllabifier.egg-info/top_level.txt -> libindic-syllabifier-0.0.1.dev28/libindic_syllabifier.egg-info\n",
            "INFO:root:copying tests/__init__.py -> libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:copying tests/indcsyllabifier_test.py -> libindic-syllabifier-0.0.1.dev28/tests\n",
            "INFO:root:Writing libindic-syllabifier-0.0.1.dev28/setup.cfg\n",
            "INFO:root:Creating tar archive\n",
            "INFO:root:removing 'libindic-syllabifier-0.0.1.dev28' (and everything under it)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dist/libindic-syllabifier*.tar.gz\n",
        "!pip install libindic-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "x3tCAUdVLnRY",
        "outputId": "22312c00-d83f-4288-d932-ec2b4bad0403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./dist/libindic-syllabifier-0.0.1.dev28.tar.gz\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: silpa_common in /usr/local/lib/python3.10/dist-packages (from libindic-syllabifier==0.0.1.dev28) (0.3)\n",
            "Building wheels for collected packages: libindic-syllabifier\n",
            "  Building wheel for libindic-syllabifier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libindic-syllabifier: filename=libindic_syllabifier-0.0.1.dev28-py2.py3-none-any.whl size=8121 sha256=78ff2e49a43f590cd0640920546a28ae41a407fdb5f76efe3eb3f8db7ab6ef78\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/50/19/e2620530fc718493df9f5bca90d23db025cc864b9c7c0a98ef\n",
            "Successfully built libindic-syllabifier\n",
            "Installing collected packages: libindic-syllabifier\n",
            "  Attempting uninstall: libindic-syllabifier\n",
            "    Found existing installation: libindic-syllabifier 0.0.1.dev28\n",
            "    Uninstalling libindic-syllabifier-0.0.1.dev28:\n",
            "      Successfully uninstalled libindic-syllabifier-0.0.1.dev28\n",
            "Successfully installed libindic-syllabifier-0.0.1.dev28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "libindic"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: libindic-utils in /usr/local/lib/python3.10/dist-packages (1.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from libindic.syllabifier import Syllabifier\n",
        "instance = Syllabifier()"
      ],
      "metadata": {
        "id": "7tzUFkWJLnOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable(word):\n",
        "    try:\n",
        "        sylbls = instance.syllabify(word)\n",
        "    except:\n",
        "        sylbls = list(word)\n",
        "    return sylbls"
      ],
      "metadata": {
        "id": "KtgATuZfM1y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate word n-grams\n",
        "\n",
        "def generate_N_grams(text, ngram=1):\n",
        "    words = [word for word in text.split(\" \")]  # if word not in set(stopwords.words('english'))]\n",
        "    temp = zip(*[words[i:] for i in range(0, ngram)])\n",
        "    ngrams = [' '.join(ngram) for ngram in temp]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "--zdcWzrM1v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex\n",
        "\n",
        "def custom_analyzer(text):\n",
        "    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n",
        "    for w in words:\n",
        "        yield w"
      ],
      "metadata": {
        "id": "4U90T1fXM1tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate syllable n-grams by specifying the value of n, by default ngram=1\n",
        "\n",
        "def generate_cN_grams(word, ngram=1):\n",
        "    sylbs = [sylbs for sylbs in syllable(word)]  # if word not in set(stopwords.words('english'))]\n",
        "    temp = zip(*[sylbs[i:] for i in range(0, ngram)])\n",
        "    ngrams = [''.join(ngram) for ngram in temp]\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "ZvURcs8zM1qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate syllable n-grams in the range 'from' to 'to'. by default 'from'= 1\n",
        "\n",
        "def ngrams(text, r_to, r_from=1):\n",
        "    r_from = 1\n",
        "    r_to = 3\n",
        "    allngrams=[]\n",
        "    for word in text.split():\n",
        "        temp = []\n",
        "        for i in range(r_from + r_to - 1):\n",
        "            temp.extend(generate_cN_grams(word, i+1))\n",
        "            allngrams.append(temp)\n",
        "    n_grams = [item for sublist in allngrams for item in sublist]\n",
        "  # merged = list(itertools.chain(allngrams))\n",
        "    return n_grams"
      ],
      "metadata": {
        "id": "lZfZmiLUM1oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_analyzer(text):\n",
        "    syngrams = ngrams(text,3)\n",
        "    return syngrams"
      ],
      "metadata": {
        "id": "vmFsgPruM1lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(analyzer=custom_analyzer)\n",
        "tfidf_vectorizer.fit_transform(X_train)\n",
        "xtrain_tfidf =  tfidf_vectorizer.transform(X_train)\n",
        "xtest_tfidf =  tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "4eVa_l_fM1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble classifier"
      ],
      "metadata": {
        "id": "GmlFMII5WpG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1)\n",
        "clf2 = BernoulliNB()\n",
        "clf3 = LinearSVC(penalty='l2',C=1.0)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('LR', clf1), ('BNB', clf2),\n",
        "('SVC', clf3)], voting='hard')\n",
        "eclf = eclf.fit(xtrain_tfidf, y_train)\n",
        "y_pred_df= eclf.predict(xtest_tfidf)\n",
        "print(y_pred_df)\n",
        "\n",
        "print(\"Accuracy of %s is %s\"%(clf, acc))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred_df))\n"
      ],
      "metadata": {
        "id": "IM3Ca_lRWboW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "IAPwklNgWsF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "\n",
        "svm .fit(xtrain_tfidf, y_train)\n",
        "\n",
        "y_pred = svm.predict(xtest_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Cg3Ls_WHNYJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticRegression"
      ],
      "metadata": {
        "id": "4vxJoNZXYkOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr=LogisticRegression()\n",
        "\n",
        "lr .fit(xtrain_tfidf, y_train)\n",
        "\n",
        "y_pred = lr.predict(xtest_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "bi1bzlIXXHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestClassifier"
      ],
      "metadata": {
        "id": "-PNXMcppYnid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RFC = RandomForestClassifier\n",
        "\n",
        "RFC .fit(xtrain_tfidf, y_train)\n",
        "\n",
        "y_pred = RFC.predict(xtest_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
        "\n",
        "print(\"\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9G5NK20tXHoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}